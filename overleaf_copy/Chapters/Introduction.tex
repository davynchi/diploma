% Chapter Template

\chapter{Введение} % Main chapter title

\label{Introduction} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\hook}{\hookrightarrow}
\newcommand{\bb}[1]{\mathbb{#1}}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Мотивировка}

\subsection{Описание проблем}
\label{sec:problem_description}

Инвесторы вкладывают свои деньги в акции, чтобы сохранить или приумножить свои богатства. Представим себе, что инвестор хочет вложить деньги в $n$ акций. Будем считать, что $i$-ая акция за определенный фиксированный промежуток времени увеличивается в своей стоимости на значение случайной величины $\xi_i$ , причем $\forall \, i, \, j, \: i \neq j \hook \xi_i \perp \xi_j$. Инвестору необходимо найти оптимальный вектор $\tbf{p} \in \Delta^n$, максимизирующий получаемую в среднем субъективную прибыль от активов за один промежуток времени, скажем, за день. Для максимизации прибыли инвесторы могут использовать модели для приближенного вычисления изменения стоимости акций \cite{bouchaudpotters}. Если инвестор не обладает информацией о стоимости активов (например, актив только появился, или до этого данные об активе были засекречены), то в качестве одной моделей можно использовать модель многоруких бандитов.

Классическая задача о многоруких бандитах звучит так: есть $n$ рычагов, каждый ход можно выбрать один из рычагов, и в ответ на выбор $i$-ого рычага игрок получает награду из распределения, связанного с $i$-ым рычагом \cite{suttonbarto}. Цель -- максимизировать среднюю награду $\bb{E} R_T$ спустя $T$ шагов, например, 1000. Оптимальной стратегией является нажатие на лучшие рычаги, то есть рычаги с наибольшим матожиданием (в случае, если нам неинтересны риски) или, если матожидания нет, с наибольшим значением по какой-то единой для всех рычагов метрике, например, по медиане. Проблема заключается в том, что изначально распределения наград для рычагов неизвестны, и их надо приблизить нажатиями на рычаги и получением выборки наград.

В таком случае задача инвестора -- последовательными изменениями своего экономического портфеля и  ``нажатиями на рычаги'' найти такое распределение денег по акциям, которое в среднем дает наибольший прирост стоимости портфеля. Для нахождения распределения денег можно использовать стандартные алгоритмы из задачи о многоруких бандитах.

В предложенной схеме есть несколько проблем:
\begin{enumerate}
    \item В качестве распределений рычагов (то есть распределений изменений активов в цене) чаще всего берется гауссовское распределение, в то время как в реальной жизни гораздо чаще наблюдаются степенные распределения с более тяжелыми хвостами.
    \item Кроме матожидания, инвесторы также заинтересованы в учете рисков при вкладывании денег в актив, поскольку портфель должен быть предсказуемым. Классическая постановка задачи о многоруких бандитах учитывает только матожидание (то есть среднюю награду) каждого рычага, совершенно не учитывая риски.
    \item В качестве распределений рычагов для решения задачи о многоруких бандитах берутся распределения из одного семейства, например, из семейства нормальных случайных величин. В реальном мире, естественно, изменения активов могут быть подчинены распределениям не из одного семейства.
\end{enumerate}

\subsection{Подход к решению проблем}

Для упрощения задачи в этой работе опущена треться проблема, то есть считается. что распределения берутся из одного семейства. Что касается остального, то здесь естьь способы их решения.

Для решения первой проблемы было решено проверить классические алгоритмы из задачи о многоруких бандитах, такие как $\epsilon$-greedy, Positive Initialisation, Upper-Confidence Bound (UCB), Gradient Bandits, для распределений Стьюдента с разными степенями свободы. Распределение Стьюдента было взято по нескольким причинам. Во-первых, его хвосты подчинены степенному распределению, что близко к реальному изменению цен активов. Во-вторых, изменение параметра числа степеней свободы $\nu$ задает меру хаотичности распределения: от отсутствия матожидания и почти полного хаоса при $\nu=1$, что соответствует распределению Коши, до упорядоченности при $\nu \to \infty$, что соответствует нормальному распределению.

Что касается второй проблемы, то необходимо задать функцию полезности, включающей риски и максимизацией которой будут заниматься алгоритмы. Мы возьмем версию функции полезности, основанной на дисперсии. Итак, есть $n$ рычагов, $i$-ый рычаг соответствует какому-то распределению со средним $m_i$ и дисперсией $\sigma_i^2$. Распределения изначально нам неизвестны. Каждый ход мы можем выбрать один из рычагов, при выборе $i$-го рычага мы получаем награду, сгенерированную из $i$-го распределения. После $t$-го шага у нас имеется вектор вероятностей $P_t = (p_1^t, ..., p_n^t)$, $\forall i \: p_i \geq 0$, и рычаг выбирается в соотвествии с этим вектором. Цель -- за $T$ шагов по получаемым наградам максимизировать $V = m_p - \lambda \cdot \sigma_p^2 = \sum_{i=1}^n p_i^T m_i - \lambda \sum_{i=1}^n (p_i^T)^2 \sigma_i^2$, то есть найти такой вектор $\textbf{p}^* \in \Delta^n$, что $\tbf{p}^* = \underset{\tbf{p} \in \Delta^n}{\arg \max} \sum_{i=1}^n p_i m_i - \lambda \sum_{i=1}^n p_i^2 \sigma_i^2$. Число $\lambda > 0$ называется степенью (коэффициентом) отвращения к риску (неприятия риска).

Почему функция полезности $V$ выглядит именно так? Будем считать, что вектор $\tbf{p}$ описывает собой доли денег, вкладываемых инвестором каждый шаг в каждый из активов. \cite{bouchaudpotters}. По-другому говоря, если инвестор решает за ход вложить $M$ денег в активы, то в $i$-ый актив будет вложено $p_i M$ денег. Для удобства положим $M=1$. Тогда матожиидание увеличения стоимости портфеля будет составлять $m_p = \sum_{i=1}^n p_i m_i$, а матожидание рисков будет равно дисперсии портфеля, то есть $\sigma_p^2 = \sum_{i=1}^n (p_i)^2 \sigma_i^2$. Таким образом, максимизация $V$ равносильна максимизации $m_p - \lambda \sigma_p^2$, то есть разности среднего увеличения портфеля и средних рисков портфеля, домноженных на степень неприятия к риску.

Для максимизации этой функции полезности $V$ были разработаны алгоритмы, находящие оптимальный вектор вероятностей $\tbf{p}$, максимизирующий $V$ при условии известных значений матожиданий и дисперсии. Далее описанные выше стратегии из обычной задачи о многоруких бандитах были адаптированы для задачи с учетом неприятия к риску, в их основу положен алгоритм нахождения оптимального вектора вероятностей. Наконец, все измененные стратегии были протестированы на распределениях Стьюдента с разным числом степеней свободы.

Дополнительно стоит отметить, что, хотя для $\nu > 2$ у распределения Стьюдента с $\nu$ степенями свободы есть дисперсия и, следовательно, определена функция полезности, при $\nu \to 2+$ могут начать происходить интересные явления. В работе проанализировано влияние этих явлений на эффективность максимизации $V$ и сделаны выводы о применимости оценки рисков с учетом дисперсии для использования в реальном мире.

\section{Обзор литературы}

Главная работа, в которой описываются основные алгоритмы для решения задачи о многоруких бандитах, является книга Ричарда Саттона и Эндрю Барто "Reinforcement Learning: An Introduction" \cite{bouchaudpotters}. Хотя описание алгоритмов снабжено многочисленными графиками и сравнениями эффективности алгоритмов, в их описании прослеживаются несколько описанных выше проблем. Во-первых, в качестве распределений для рычагов используются нормальные распределения и не рассматриваются никакие другие распределения, что является серьезным упущением. Во-вторых, поскольку в их цели не входило обширное исследование задачи о многруких бандитах, никак не был учтен риск от выбора того или иного рычага.

Другой работой, в которой исследуется вопрос максимизации функции полезности с учетом неприятия к риску, является портфельная теория Марковица, описанная, в частности, в \cite{bouchaudpotters}. В теории вычислено точное значение вектора $\tbf{p}$, а также помимо дисперсии рассмотрен другой подход к измерению риска - Value at Risk (VaR) \cite{varbouchaudpotters}. Однако при этом присутствует 2 недостатка:
\begin{enumerate}
    \item Изначально все матожидания и дисперсии считаются известными, что не всегда соответствует действительности.
    \item В оптимальном решении могут быть ШортЫ (то есть максимум функции полезности берется не по симплексу $\Delta^n$, а по всей гиперплоскости $\{(p_1, ..., p_n): \sum_{i=1}^n p_i = 1\}$). Инвесторы не всегда могут давать деньги в долг.
\end{enumerate}

Таким образом, хотя вышеперечисленные работы приводят исследования описанных в \ref{sec:problem_description} проблем, они все же обладают некоторыми недостатками, а, главное, рассматривают модели отдельно, не пытаясь объединить их в одную общую модель. Эта работа устраняет описанные проблемы, предлагая стратегии для их решения.

\section{Структура}

В главе \ref{Theory} этой работы сначала описываются стратегии для итеративного приближения оптимального решения, каждый шаг которых работает за $O(n^2)$. Затем описываются стохастические методы для нахождения оптимального вектора при условии известных матожиданий и дисперсий, основанные на градиентном подъеме. Далее предлагаются алгоритмы, работающие за $O(n \log n)$. Стандартные стратегии, такие как $\epsilon$-greedy, позитивная инициализация, UCB, Gradient bandits, изменены для оптимальной работы с учетом неприятия к риску. В основу этих стратегий положен самый удачный из алгоритмов за $O(n \log n)$. Кроме того, предложена новая стратегия, основанная на коррекции выборочной дисперсии. В главе \ref{ExperimentsClassic} проводятся экспермиенты для стандартных стратегий и различных распределений, полученные результаты визуализируются, приводится объяснение наблюдаемым явлениям, даются пргнозы по эффективности стратегий для задачи о многоруких бандитах с учетом неприятия к риску. Все стратегии сравниваются при различных гиперпараметрах. В главе \ref{ExperimentsAversion} проводятся эксперименты для разработанных во второй главе стратегий для задачи о могоруких бандитах с измененной функцией полезности, аналогично, результаты визуализируются и объясняются. Наконец, в пятой главе (ССЛЫКА!) делаются общие выводы об эффективности стратегий и о применимости в реальной жизни их и постановки задачи оптимизации портфеля, основанной на дисперсии. Кроме того, описываются дальнейшие направления развития теории.

Код, реализующий все алгоритмы, а также саму статью можно найти \href{https://github.com/davynchi/diploma/blob/main}{в репозитории} на Github.











